---
title: "03_SummerClustering"
author: "Simon Topp"
date: "8/10/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(purrr)
library(furrr)
library(data.table)
library(feather)
library(sf)
library(dtwclust)
library(networkD3)
library(tictoc)
knitr::opts_chunk$set(echo = TRUE)
```
## Try pulling out the smoothed trend lines using doy for each period

```{r}
srCor <- read_feather('data/processed/srCorrected_us_hydrolakes_dp_20200628.feather')

iterations = 'May_Oct'
## Play around with some clustering

## Take a representative sample of HydroLakes
Ecoregs <- st_read('../USLakeClarityTrendr/in/NLA/NLA_Ecoregions/EcoRegsMerged.shp')

dp <- st_read('data/in/HydroLakes_DP/HydroLakes_DP.shp')

hl <- st_read('../USLakeClarityTrendr/in/hydroLakes/HydroLAKES_polys_v10_shp/HydroLAKES_polys_v10.shp') %>%
  filter(Hylak_id %in% dp$Hylak_id) %>%
  st_centroid()


srCor <- as.data.table(srCor)[,period := cut(year,6, dig.lab = 4)
                                ][,doy := yday(date)
                                  ][, week := ifelse(week  == 53,52,week)
                                    ][,biWeek := ifelse(week %% 2 == 0,
                                                         week, week + 1)]

```


```{r}
##  Take a look at biWeek distributions to decide cut-off's for 'Summer'
hist(srCor$biWeek)
#Looks like it should be ~18 and 38, or ~May-Oct
summary(factor(srCor$biWeek))

FilterCounts <- srCor[month %in% c(5:10)
                      ][, .N, by = .(Hylak_id, period, month)]

# Find all possible combos to make sure its exhaustive
combos <- as.data.table(expand.grid(month = unique(FilterCounts$month),
                                    Hylak_id = unique(srCor$Hylak_id), 
                                    period = unique(FilterCounts$period)))

# Get rid of months with < 2 Observations
Filter <- merge(FilterCounts,combos, all = T)[is.na(N) | N < 2]

# Get rid of big outliers and scale dWL
lakeSamp <- srCor[month %in% c(5:10) & !Hylak_id %in% unique(Filter$Hylak_id),
                  .(Hylak_id, period, date, dWL, doy)
                  ][,dWL.scaled := scale(dWL), by = .(Hylak_id, period)
                    ][dWL.scaled < 4 & dWL.scaled > -4]

## This leaves us with ~26,000 lakes with at least 2 observations per month
## in each of the 6 periods
length(unique(lakeSamp$Hylak_id))

## Look at the distribution
lakeSamp %>% distinct(Hylak_id) %>% inner_join(hl) %>% st_as_sf() %>%mapview::mapview()

min(lakeSamp$doy)
max(lakeSamp$doy)

# Create a function to pass a gaussian smoother over each perdiod/lake combo
# to get 1 observation per weak.
k.smoother.summer <- function(data, period, Hylak_id){
 if(type == 'norm'){
  k <- ksmooth(data$doy, data$dWL.scaled, kernel = 'normal', 
                   bandwidth = 21, range.x = seq(min(lakeSamp$doy),max(lakeSamp$doy)), 
                   x.points = seq(min(lakeSamp$doy),max(lakeSamp$doy),7))
 }else if(type == 'raw'){
   k <- ksmooth(data$doy, data$dWL, kernel = 'normal', 
                   bandwidth = 21, range.x = seq(min(lakeSamp$doy),max(lakeSamp$doy)), 
                   x.points = seq(min(lakeSamp$doy),max(lakeSamp$doy),7))
 }
 
 tibble(period = period, Hylak_id = Hylak_id, doy = k$x, smoothed = k$y) %>%
  arrange(doy) %>%
  pivot_wider(names_from = doy, values_from = smoothed)
}


# Nest it all as a data.table
.nest <- function(...) list(data.table(...))

smoothed <- lakeSamp[, .(data = .nest(.SD)),  by = .(period, Hylak_id)]

## Heads up, this takes about 25 minutes
type = 'norm'
tic()
plan(multiprocess, workers = 4)
smoothed.norm <- smoothed %>% future_pmap_dfr(k.smoother.summer, .progress = T)
plan(sequential)
toc()

type = 'raw'
tic()
plan(multiprocess, workers = 4)
smoothed.raw <- smoothed %>% future_pmap_dfr(k.smoother.summer, .progress = T)
plan(sequential)
toc()

rm(smoothed, type)
write_feather(smoothed.norm, paste0('data/out/',iterations,'_Wide_Normalized.feather'))
write_feather(smoothed.raw, paste0('data/out/',iterations,'_Wide_Raw.feather'))
```


```{r}
smoothed.norm <- read_feather(paste0('data/out/',iterations,'_Wide_Normalized.feather'))
smoothed.raw <- read_feather(paste0('data/out/',iterations,'_Wide_Raw.feather'))

vis <- smoothed.norm %>% distinct(Hylak_id) %>% sample_n(5)

vis <- smoothed.norm %>% filter(Hylak_id %in% c(1050554, 105672, 1022450, 106100, 1061496)) %>% #vis$Hylak_id) %>%
  pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'dWL') %>%
  mutate(doy = as.numeric(doy))

ggplot(vis, aes(x = doy)) + 
  geom_point(data = lakeSamp %>% 
               filter(Hylak_id %in% vis$Hylak_id), 
             aes(color = 'black', y = dWL.scaled), alpha = .7) +
  geom_line(size = 1, aes(y = dWL, color = 'red'), show.legend = T) +
  scale_color_identity(guide = 'legend',
                       name = '',
                       breaks = c('black', 'red'),
                       labels = c('Observed Color','Kmeans Climatology')) +
  theme_bw() +
  coord_cartesian(ylim = c(-2,2)) +
  theme(legend.position = 'top') +
  labs(x = 'Day of Year', y = 'Z-Normalized Dominant Wavelength') +
  facet_grid(Hylak_id~period, scales = 'free')


ggsave('figures/ClimatologyExample.png', width = 6.5, units = 'in')
```


```{r}
## Check for mixxing values
colSums(is.na(smoothed.norm))

##Take a quick look at the distribution
#smooothed.sf <- hl %>% inner_join(smoothed) %>% distinct(Hylak_id, .keep_all = T)
#mapview(smoothed.sf)

## There's some randomness in partitional clustering, so set the seed beforehand
smoothed.check <- smoothed.norm %>% filter(Hylak_id %in% hl$Hylak_id)


set.seed(4325)
cluster_dtw<-tsclust(smoothed.norm %>% select(-Hylak_id, -period), 
                     type = "p", k = 2L:8L,#2L:8L, 
                     distance = "dtw_basic",
                     centroid = 'dba',
                     #window.size = 2L,
                     #control = hierarchical_control(method = 'complete'),
                     control = partitional_control(iter.max = 400L),
                     trace = T,
                     args = tsclust_args(dist = list(window.size = 2L),
                                         cent = list(window.size = 2L)),
                     preproc = NULL)

# extract the cluster validation indices
# The vignette to dtwclust mentions that CH and SF may not be appropriate with 
# dba centroids, so we'll exclude them. Similarly, Sil, Dunn, and COP, all require the entire
# cross-distance matrix which is way too big with the number of samples we have.
# That leaves DB and Modified DB
cvi_dw <-sapply(cluster_dtw, cvi, type = c('DB','DBstar')) #c('D','Sil', 'D', 'COP', 'DB', 'DBstar')) 

cvi_names <-rownames(cvi_dw)

cvi_df <- cvi_dw %>%
  as_tibble() %>%
  cbind(cvi_names) %>%
  pivot_longer(-cvi_names, names_to="clusters", names_prefix = "V",values_to = "CVI" ) %>%
  mutate(clusters = as.numeric(clusters) +1)

cvi.selects <- cvi_df %>% group_by(cvi_names) %>%
  mutate(select= ifelse(cvi_names %in% c('CH', 'D', 'SF', 'Sil'), clusters[CVI == max(CVI)], clusters[CVI == min(CVI)]),
         CVI = ifelse(cvi_names %in% c('CH', 'D', 'SF', 'Sil'), max(CVI), min(CVI))) %>%
  select(-clusters) %>% distinct(cvi_names, .keep_all = T)

ggplot(cvi_df) +
  geom_line(aes(x=clusters, y=CVI)) +
  geom_point(data = cvi.selects, aes(x = select, y = CVI)) +
  facet_wrap(~cvi_names, scales="free")

# Take a look at the resulting best cluster
plot(cluster_dtw[[2]], type = "centroid") 
summary(factor(cluster_dtw[[2]]@cluster))

#Save the resulting best cluster
cluster <- cluster_dtw[[2]]

save(cluster, cvi_df, file = paste0('data/out/',iterations,'_SummerClusterings.RData'))
rm(cluster_dtw)
```


## Look at the stability of each cluster

```{r}
load(paste0('data/out/',iterations,'_SummerClusterings.RData'))
labels.og <- cluster@cluster
summary(factor(labels.og))

Jaccard = function (x,y,z) {
    M.11 = sum(x == z & y == z)
    M.10 = sum(x == z & y != z)
    M.01 = sum(x != z & y == z)
    return (M.11 / (M.11 + M.10 + M.01))
}

dtw_wrapper <- function(bootsrapped){
  
  x.resample <- smoothed.norm %>% select(-Hylak_id, -period) %>% sample_n(nrow(smoothed), replace = T)
  
  clust.resample<-tsclust(x.resample, 
                     type = "p", k = 3L, 
                     distance = "dtw_basic",
                     centroid = 'dba',
                     control = partitional_control(iter.max = 400L),
                     trace = T,
                     args = tsclust_args(dist = list(window.size = 2L),
                                         cent = list(window.size = 2L)),
                     preproc = NULL)

  labels.new <- predict(clust.resample, smoothed.norm %>% select(-Hylak_id, -period)) %>% unname()
  
    ## Some hueristic iteration shows that the clusters centroids are always the three distinct groups with similar group sizes, but their order isn't always the same. So re-order based on group size
  
  relabler <- table(labels.og, labels.new) %>% 
    as_tibble() %>% 
    arrange(labels.og, -n) %>%
    distinct(labels.og, .keep_all = T)
  
  labels.new <- factor(labels.new, levels = relabler$labels.new, 
                       labels = relabler$labels.og)
  
  
  tibble(SpGr = Jaccard(labels.og, labels.new, 1),
  SuGr = Jaccard(labels.og, labels.new, 2),
  Bi = Jaccard(labels.og, labels.new, 3),
  round = bootsrapped)
}

j.stability <- c(1:100) %>% map_dfr(dtw_wrapper)

write_feather(j.stability, paste0('data/out/JaccardStability', iterations,'.feather'))

j.summary <- j.stability %>% pivot_longer(-round, names_to = 'cluster', values_to = 'score') %>% group_by(cluster) %>% summarise(mean.Jaccard = mean(score, na.rm = T), sd.Jaccard = sd(score, na.rm = T))

```


```{r}
load(paste0('data/out/',iterations,'_SummerClusterings.RData'))

# "CH" (~): Calinski-Harabasz index (Arbelaitz et al. (2013); to be maximized).
# "COP" (!): COP index (Arbelaitz et al. (2013); to be minimized).
# "D" (!): Dunn index (Arbelaitz et al. (2013); to be maximized).
# "DB" (?): Davies-Bouldin index (Arbelaitz et al. (2013); to be minimized).
# "DBstar" (?): Modified Davies-Bouldin index (DB*) (Kim and Ramakrishna (2005); to be minimized).
# "SF" (~): Score Function (Saitta et al. (2007); to be maximized; see notes).
# "Sil" (!): Silhouette index (Rousseeuw (1987); to be maximized).

clusters.sf <- hl %>% inner_join(
  tibble(Hylak_id = smoothed.norm$Hylak_id, period = smoothed.norm$period, 
         cluster = factor(cluster@cluster, levels = c(1:3), 
                          labels = c('Spring Greening', 'Summer Greening', 'Bimodal')))) 


# lakeSamp.base <- merge(
#   srCor[month %in% c(5:10) & Hylak_id %in% unique(smoothed$Hylak_id),
#         .(Hylak_id, period, week, date, dWL, doy)],
#   as.data.table(clusters.sf %>% select(Hylak_id, period, cluster)))
# 
# iqrs <- lakeSamp.base[,.(iqr75 = quantile(dWL, .75), 
#                          iqr25 = quantile(dWL, .25),
#                          med_dWL = median(dWL)), by = .(cluster, week)]
# 
# p1 <- ggplot(iqrs, aes(x = week *7)) + 
#   geom_ribbon(aes(ymin = iqr25, ymax = iqr75), fill = 'grey70') +
#   geom_line(aes(y = med_dWL)) +
#   facet_wrap(~cluster, ncol = 1) + 
#   labs(title = 'Raw', y = 'Dominant Wavelength') +
#   theme_bw() +
#   theme(axis.title = element_blank())


#############

clusters.raw.long <- smoothed.raw %>% pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'dWL') %>% as.data.table()

clusters.raw.long <- merge(clusters.raw.long, as.data.table(clusters.sf %>% select(Hylak_id, period, cluster)))

clusters.raw.long <- merge(clusters.raw.long, fui.lookup, by = 'dWL')

iqrs.raw <- clusters.raw.long[,.(iqr75 = quantile(dWL, .75), 
                         iqr25 = quantile(dWL, .25),
                         med_dWL = median(dWL)), by = .(cluster, doy)
                         ][,doy := as.numeric(doy)]

cl = 'Spring Greening'
getDense <- function(cl){
  df <- clusters.raw.long[clusters.raw.long$cluster == cl]
  dense <- density(df$dWL, n = 200)
  tibble(cluster = cl, dWL = as.integer(dense$x), dense = dense$y) %>% inner_join(fui.lookup, by = 'dWL')
  }
  
ColorDense = levels(clusters.raw.long$cluster) %>% map_dfr(getDense) %>%
  mutate(cluster = factor(cluster, levels = c('Spring Greening', 'Summer Greening', 'Bimodal')))



p1 <- ggplot(ColorDense, aes(x = dWL, y = dense, color = fui)) +
  geom_segment(aes(xend = dWL, yend = 0), size = 2) +
  scale_color_gradientn(colors = fui.colors) +
  labs(tag = 'c', x = expression(lambda[d]), title = 'Density') +
  theme(axis.line.y = element_blank(),
      axis.ticks = element_blank(),
      #axis.text.x = element_text(vjust = 5),
      axis.text.y = element_blank(),
      axis.title.y = element_blank(),
      panel.background = element_blank(),
      panel.grid = element_blank(),
      legend.position = 'none') +
  facet_grid(rows = vars(cluster))


p2 <- ggplot(iqrs.raw, aes(x = doy)) + 
  geom_ribbon(aes(ymin = iqr25, ymax = iqr75), fill = 'grey70') +
  geom_line(aes(y = med_dWL)) +
  facet_wrap(~cluster, ncol = 1) +
  labs(tag = 'b', title = 'Raw') +
  theme_bw() +
  theme(axis.title = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())

p2

###############
clusters.long <- smoothed.norm %>% pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'norm_dWL') %>% as.data.table()

clusters.long <- merge(clusters.long, as.data.table(clusters.sf %>% select(Hylak_id, period, cluster)))

iqrs.norm <- clusters.long[,.(iqr75 = quantile(norm_dWL, .75), 
                         iqr25 = quantile(norm_dWL, .25),
                         med_dWL = median(norm_dWL)), by = .(cluster, doy)
                         ][,doy := as.numeric(doy)]

p3 <- ggplot(iqrs.norm, aes(x = doy)) + 
  geom_ribbon(aes(ymin = iqr25, ymax = iqr75), fill = 'grey70') +
  geom_line(aes(y = med_dWL)) +
  facet_wrap(~cluster, ncol = 1) +
  labs(tag = "a", title = 'Z-Normalized', y = expression(lambda[d])) +
  theme_bw() +
  theme(axis.title.x = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
p3

g1 <- gridExtra::grid.arrange(p3,p2,ncol = 2, widths = c(1.1,1), bottom = 'Day of Year')

g <- gridExtra::grid.arrange(g1, p1, ncol = 2, widths = c(1,.5))

ggsave('figures/ClusterIQRs.png',plot = g, width = 6, height = 5, units = 'in')


# Just pull some summary values
Modes <- function(x) {
  ux <- unique(x)
  tab <- tabulate(match(x, ux))
  ux[tab == max(tab)][1]
  }


clusters.raw.long %>% group_by(cluster) %>% 
  summarise(
    mean = mean(dWL),
    median = median(dWL),
    sd = sd(dWL),
    IQR = IQR(dWL),
    mode = Modes(dWL),
    var = var(dWL))

#Summary variation by lake
Test <- clusters.raw.long %>% group_by(cluster, period, Hylak_id) %>%
  summarise(sd = sd(dWL),
            cv = raster::cv(dWL),
            rangeLow = range(dWL)[1],
            rangeHigh = range(dWL)[2])

Test %>%
  group_by(cluster) %>%
  summarise(mean.sd = mean(sd),
            mean.cv = mean(cv),
            sd.sd = sd(sd),
            mean.rl = mean(rangeLow),
            mean.rh = mean(rangeHigh),
            range = mean.rh - mean.rl)

ggplot(Test, aes(x = cv)) + geom_density() + facet_wrap(~cluster)

kruskal.test(cv ~ cluster, Test)
FSA::dunnTest(cv~cluster, data = Test, method = 'bonferroni')

FSA::dunnTest(dWL~cluster, data = clusters.raw.long, method = 'bonferroni')

wilcox.test(lakeSamp$dWL[clusters.raw.long$cluster == 'Bimodal'],
            clusters.raw.long$dWL[clusters.raw.long$cluster!='Bimodal'], conf.int = T)


fligner.test(dWL ~ cluster, clusters.raw.long[clusters.raw.long$cluster != 'Spring Greening'])
fligner.test(dWL ~ cluster, clusters.raw.long[clusters.raw.long$cluster != 'Summer Greening'])
fligner.test(dWL ~ cluster, clusters.raw.long[clusters.raw.long$cluster != 'Bimodal'])
```


## Look at spatial distribution of clusters

```{r}
## By period
usa <- maps::map('usa', plot = F) %>% st_as_sf() %>% st_transform(st_crs(hl)) 

grid <- st_make_grid(usa, cellsize = c(100000,100000), square = F) %>% st_as_sf() %>% mutate(ID = row_number())
 
grid <- grid %>% st_join(clusters.sf, left = F)

Modes <- function(x) {
  ux <- unique(x)
  tab <- tabulate(match(x, ux))
  mode <- ux[tab == max(tab)]
  mode <- ifelse(length(mode) > 1, 'mixed', as.character(mode))
  return(as.factor(mode))
}

modalClust <- grid %>% st_set_geometry(NULL) %>%
  group_by(ID, period) %>%
  summarise(modalClust = Modes(cluster))

grid <- grid %>% inner_join(modalClust %>% mutate(modalClust = factor(modalClust, levels = c('1','2','3','mixed'), labels = c('Bimodal', 'Summer Greening', 'Spring Greening', 'Mixed'))))

ggplot() + 
  geom_sf(data = usa) +
  geom_sf(data = grid, aes(fill = modalClust)) +
  scale_fill_viridis_d() +
  ggthemes::theme_map(base_family = 'Arial', base_size = 12) +
  theme(legend.position = 'bottom') +
  facet_wrap(~period)

ggsave('figures/DominantCluster.png', width = 6.5, height = 4, units = 'in')


## Overall
grid <- st_make_grid(usa, cellsize = c(50000,50000), square = F) %>% st_as_sf() %>% mutate(ID = row_number())
 
grid <- grid %>% st_join(clusters.sf, left = F)

modalClust <- grid %>% st_set_geometry(NULL) %>%
  group_by(ID) %>%
  summarise(modalClust = Modes(cluster))

grid <- grid %>% inner_join(modalClust %>% mutate(modalClust = factor(modalClust, levels = c('1','2','3','mixed'), labels = c('Bimodal', 'Summer Greening', 'Spring Greening', 'Mixed'))))

ggplot() + 
  geom_sf(data = usa) +
  geom_sf(data = grid, aes(fill = modalClust)) +
  scale_fill_viridis_d('Modal\nCluster', labels = c('Bimodal', 'Summer\nGreening','Spring\nGreening', 'Multiple\nModes'), option = 'plasma') +
  ggthemes::theme_map(base_size = 11) +
  theme(legend.position = 'bottom')

ggsave('figures/DominantCluster.png', width = 4.5, height = 4, units = 'in')

```


## Try to make a transition plot

```{r}
links.base <- clusters.sf %>% st_set_geometry(NULL) %>%
  #mutate(cluster = paste0('Cluster_',cluster)) %>%
  #mutate(pClust = paste0('c',cluster,'_',period), count = 1) %>%
  select(Hylak_id, period, cluster) %>%
  arrange(period) %>%
  pivot_wider(names_from = period, values_from = cluster) 

links.base[is.na(links.base)] = 'None'

p1Tab <- as.data.frame(table('source' = links.base[[2]],'target' = links.base[[3]])) %>%
  mutate(source = paste0('p1_',source), target = paste0('p2_',target))

p2Tab <- as.data.frame(table('source' = links.base[[3]],'target' = links.base[[4]])) %>%
  mutate(source = paste0('p2_',source), target = paste0('p3_',target))

p3Tab <- as.data.frame(table('source' = links.base[[4]],'target' = links.base[[5]])) %>%
  mutate(source = paste0('p3_',source), target = paste0('p4_',target))

p4Tab <- as.data.frame(table('source' = links.base[[5]],'target' = links.base[[6]])) %>%
  mutate(source = paste0('p4_',source), target = paste0('p5_',target))

p5Tab <- as.data.frame(table('source' = links.base[[6]],'target' = links.base[[7]])) %>%
  mutate(source = paste0('p5_',source), target = paste0('p6_',target))

links <- bind_rows(p1Tab,p2Tab,p3Tab,p4Tab,p5Tab)

nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  ) %>%
  mutate(nodeGroup = ifelse(grepl('Spring',name), 1, 
                            ifelse(grepl('Summer',name), 2, 
                                   ifelse(grepl('Bimodal',name),3,
                                          ifelse(grepl('_4',name),4,
                                                 ifelse(grepl('_5',name),5,
                                                              ifelse(grepl('_6',name),6,7)))))),
         nodeGroup = factor(nodeGroup))
 
# With networkD3, connection must be provided using id, not using real name like in the links dataframe.. So we need to reformat it.
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1
 
# Make the Network

cs <- 'd3.scaleOrdinal() .range(["#440154FF", "#2A788EFF", "#7AD151FF"])'
p <- sankeyNetwork(Links = links, Nodes = nodes,
                     Source = "IDsource", Target = "IDtarget",
                     Value = "Freq", NodeID = "name", NodeGroup = 'nodeGroup',
                     sinksRight=F, nodeWidth = 10, nodePadding = 30,
                   colourScale = cs, fontSize = 12, fontFamily = 'Arial')

p

## Pull out summary stats
links.summary <- links %>% left_join(nodes, by = c('source' = 'name')) %>%
  rename(nodeSource = nodeGroup) %>%left_join(nodes, by = c('target' = 'name')) %>%
  rename(nodeTarget = nodeGroup) %>%
  mutate(PChange = Freq/sum(p1Tab$Freq)) %>%
  group_by(nodeTarget, nodeSource) %>%
  summarise(mean = mean(PChange),
            sd = sd(PChange))
  
interVintra <- links.summary <- links %>% left_join(nodes, by = c('source' = 'name')) %>%
  rename(nodeSource = nodeGroup) %>%left_join(nodes, by = c('target' = 'name')) %>%
  rename(nodeTarget = nodeGroup) %>%
  mutate(PChange = Freq/sum(p1Tab$Freq)) %>%
  mutate(group = ifelse(nodeSource == nodeTarget, 'intra', 'inter'),
         group = paste0(nodeSource, group))

kruskal.test(PChange ~ group, interVintra)
dt <- FSA::dunnTest(PChange~group, data = interVintra, method = 'bonferroni')
print(dt,  dunn.test.results = T)


 
links.base <- links.base %>%
  mutate(change1 = ifelse(`(1984,1990]` == `(1990,1996]`, 0,1),
         change2 = ifelse(`(1990,1996]` == `(1996,2002]`, 0,1),
         change3 = ifelse(`(1996,2002]` == `(2002,2008]`, 0,1),
         change4 = ifelse(`(2002,2008]` == `(2008,2014]`, 0,1),
         change5 = ifelse(`(2008,2014]` == `(2014,2020]`, 0,1),
         totalChange = factor(change1 + change2 + change3 + change4 + change5))

summary(links.base$totalChange)

stability <- links.base %>% 
  pivot_longer(`(1984,1990]`:`(2014,2020]`, names_to = 'Period', 
               values_to = 'Cluster') %>% 
  group_by(Hylak_id) %>% 
  summarize(States = factor(length(unique(Cluster)), 
                            levels = c(1:3), labels = c(1:3))) %>%
  left_join(links.base %>% select(Hylak_id, totalChange))
 
links.sf <- stability %>% inner_join(hl) %>% st_as_sf() 
links.base <- links.sf %>% st_set_geometry(NULL)
mapview::mapview(links.sf, zcol = 'States')

var = 'Depth_avg'

wilcox.test(links.base[[var]][links.base$States == 1], links.base[[var]][links.base$States == 3])

p1 <- ggplot(links.sf, aes(x = totalChange, y = Vol_total)) +
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Volume (km3)') + 
  coord_cartesian(ylim = c(.1,50)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size=4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p2 <- ggplot(links.sf, aes(x = totalChange, y = Res_time)) + 
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Residence Time') + 
  coord_cartesian(ylim = c(10,10000)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size=4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p3 <- ggplot(links.sf, aes(x = totalChange, y = pop_sum_mean)) + 
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  scale_y_continuous(trans = 'log10') +
  labs(x = 'Stability', y = 'Catchment Population') + 
  coord_cartesian(ylim = c(50,15000)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size=4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p4 <- ggplot(links.sf, aes(x = totalChange, y = Slope_100)) + 
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  #scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Surrounding Slope') + 
  coord_cartesian(ylim = c(.5,4)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size= 4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p5 <- ggplot(links.sf, aes(x = totalChange, y = Depth_avg)) + 
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  #scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Average Depth (m)') + 
  coord_cartesian(ylim = c(2,8)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size= 4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p6 <- ggplot(links.sf, aes(x = totalChange, y = total_precip_mm_mean)) +
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Mean Annual Precip (mm)') + 
  coord_cartesian(ylim = c(3000, 10000)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size = 4, color="red", fill="red")  +
  theme(axis.title.x = element_blank())

g <- gridExtra::grid.arrange(p2,p3,p5,p6, bottom = 'Stability')

ggsave('figures/LakeLandscapeMetrics.png', plot = g, width = 4, height = 4, units = 'in')
## Bring in the GLCP
#glcp <- read_csv('data/in/glcp.csv')
#glcp.filt <- as.data.table(glcp)[Hylak_id %in% unique(smoothed$Hylak_id)]
#write_feather(glcp.filt, 'data/in/glcpFiltered.feather')
glcp <- read_feather('data/in/glcpFiltered.feather') %>%
  mutate(spRatio = seasonal_km2/permanent_km2)

glcp.avg <- glcp %>% group_by(Hylak_id) %>% summarise_at(vars(total_precip_mm:pop_sum, seasonal_km2, spRatio), c(mean = mean, sd = sd), na.rm = T)

links.sf <- links.sf %>% left_join(glcp.avg)


ggplot(links.sf, aes(x = totalChange, y = Wshd_area)) +
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  scale_y_continuous(trans = 'log10') + 
  #labs(x = 'Stability', y = 'Mean Annual Precip (mm)') + 
  coord_cartesian(ylim = c(.1, 100)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size = 4, color="red", fill="red")  +
  theme(axis.title.x = element_blank())

## Take another stab at her incorporating number of states too.
ggplot(links.sf) + geom_bar(aes(x = totalChange, fill = States))
ggplot(links.sf) + geom_sf(aes(color = as.numeric(States))) +
  scale_color_viridis_c()


usa <- maps::map('usa', plot = F) %>% st_as_sf() %>% st_transform(st_crs(hl)) 

grid <- st_make_grid(usa, cellsize = c(50000,50000), square = F) %>% st_as_sf() %>% mutate(ID = row_number())
 
grid <- grid %>% st_join(links.sf, left = F)

modalClust <- grid %>% st_set_geometry(NULL) %>%
  group_by(ID) %>%
  summarise(modalClust = Modes(totalChange))

modalClust <- grid %>% inner_join(modalClust %>% mutate(modalClust = factor(modalClust, levels = c('0','1','2','3','4','5','mixed'), labels = c('0','1','2','3','4','5','mixed'))))

ggplot() + 
  geom_sf(data = usa) +
  geom_sf(data = modalClust, aes(fill = modalClust)) +
  scale_fill_manual(values = c(br(6), 'grey30'), 'Mode State \nChanges') +
  ggthemes::theme_map(base_size = 11) +
  theme(legend.position = 'bottom')



ggplot(links.sf, aes(x = totalChange, y = total_precip_mm_mean)) + geom_boxplot() + scale_y_continuous(trans = 'log10') + labs(x = 'Stability') + coord_cartesian(ylim = c(3000,10000))
```

```{r}
library(randomForest)
df.rf <- links.sf %>% st_set_geometry(NULL) %>% na.omit() %>%
  select(totalChange, Lake_type, Lake_area:Wshd_area, total_precip_mm_mean:spRatio_sd) %>%
  mutate(Lake_type = factor(Lake_type),
         uniqueID = row_number())

train <- df.rf %>% group_by(totalChange) %>% sample_frac(.7)
test <- df.rf %>% filter(!uniqueID %in% train$uniqueID)
train <- train %>% select(-uniqueID)

rf = randomForest(totalChange ~ ., data=train, ntree=100, mtry=2, importance=TRUE)
rf
varImpPlot(rf)

prediction_for_table <- predict(rf, test %>% select(-uniqueID, States))


caret::confusionMatrix(prediction_for_table, test$States)

```
