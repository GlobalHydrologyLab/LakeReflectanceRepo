---
title: "03_SummerClustering"
author: "Simon Topp"
date: "8/10/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(purrr)
library(furrr)
library(data.table)
library(feather)
library(sf)
library(dtwclust)
library(networkD3)
library(tictoc)
knitr::opts_chunk$set(echo = TRUE)
```
## Try pulling out the smoothed trend lines using doy for each period

```{r}
srCor <- read_feather('data/processed/srCorrected_us_hydrolakes_dp_20200628.feather')

iterations = 'May_Oct'
## Play around with some clustering

## Take a representative sample of HydroLakes
Ecoregs <- st_read('../USLakeClarityTrendr/in/NLA/NLA_Ecoregions/EcoRegsMerged.shp')

hl <- st_read('../USLakeClarityTrendr/in/hydroLakes/HydroLAKES_polys_v10_shp/HydroLAKES_polys_v10.shp') %>%
  filter(Country == "United States of America",
         Hylak_id %in% srCor$Hylak_id) %>%
  st_centroid() %>%
  st_transform(st_crs(Ecoregs)) %>%
  st_join(Ecoregs, left = F)



srCor <- as.data.table(srCor)[,period := cut(year,6, dig.lab = 4)
                                ][,doy := yday(date)
                                  ][, week := ifelse(week  == 53,52,week)
                                    ][,biWeek := ifelse(week %% 2 == 0,
                                                         week, week + 1)]
#[,biWeek := ifelse(week == 53, 52, 
#                                                      ifelse(week %% 3 == 0,
#                                                         week, week - week%%3 + 3))]

#
```


```{r}
##  Take a look at biWeek distributions to decide cut-off's for 'Summer'
hist(srCor$biWeek)
#Looks like it should be ~18 and 38, or ~May-Oct
summary(factor(srCor$biWeek))

FilterCounts <- srCor[month %in% c(5:10)
                      ][, .N, by = .(Hylak_id, period, month)]

# Find all possible combos to make sure its exhaustive
combos <- as.data.table(expand.grid(month = unique(FilterCounts$month),
                                    Hylak_id = unique(srCor$Hylak_id), 
                                    period = unique(FilterCounts$period)))

# Get rid of months with < 2 Observations
Filter <- merge(FilterCounts,combos, all = T)[is.na(N) | N < 2]

# Get rid of big outliers and scale dWL
lakeSamp <- srCor[month %in% c(5:10) & !Hylak_id %in% unique(Filter$Hylak_id),
                  .(Hylak_id, period, date, dWL, doy)
                  ][,dWL := scale(dWL), by = .(Hylak_id, period)
                    ][dWL < 4 & dWL > -4]

## This leaves us with ~26,000 lakes with at least 2 observations per month
## in each of the 6 periods
length(unique(lakeSamp$Hylak_id))

## Look at the distribution
lakeSamp %>% distinct(Hylak_id) %>% inner_join(hl) %>% st_as_sf() %>%mapview::mapview()

min(lakeSamp$doy)
max(lakeSamp$doy)

# Create a function to pass a gaussian smoother over each perdiod/lake combo
# to get 1 observation per weak.
k.smoother.summer <- function(data, period, Hylak_id){
 k <- ksmooth(data$doy, data$dWL, kernel = 'normal', 
                   bandwidth = 21, range.x = seq(min(lakeSamp$doy),max(lakeSamp$doy)), 
                   x.points = seq(min(lakeSamp$doy),max(lakeSamp$doy),7))
 
 tibble(period = period, Hylak_id = Hylak_id, doy = k$x, smoothed = k$y) %>%
  arrange(doy) %>%
  pivot_wider(names_from = doy, values_from = smoothed)
 
}

# Nest it all as a data.table
.nest <- function(...) list(data.table(...))

smoothed <- lakeSamp[, .(data = .nest(.SD)),  by = .(period, Hylak_id)]

## Heads up, this takes about 25 minutes
tic()
plan(multiprocess, workers = 4)
test <- smoothed %>% future_pmap_dfr(k.smoother.summer, .progress = T)
plan(sequential)
toc()

write_feather(test, paste0('data/out/',iterations,'_Wide_Normalized.feather'))

vis <- smoothed %>% distinct(Hylak_id) %>% sample_n(5)

vis <- smoothed %>% filter(Hylak_id %in% vis$Hylak_id) %>%
  pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'dWL') %>%
  mutate(doy = as.numeric(doy))

ggplot(vis, aes(x = doy, y = dWL)) + 
  geom_point(data = lakeSamp %>% 
               filter(Hylak_id %in% vis$Hylak_id), 
             aes(color = 'black'), alpha = .7) +
  geom_line(size = 1, aes(color = 'red'), show.legend = T) +
  scale_color_identity(guide = 'legend',
                       name = '',
                       breaks = c('black', 'red'),
                       labels = c('Observed Color','Kmeans Climatology')) +
  theme_bw() +
  theme(legend.position = 'top') +
  labs(x = 'Day of Year', y = 'Z-Normalized Dominant Wavelength') +
  facet_grid(period~Hylak_id, scales = 'free')


ggsave('figures/ClimatologyExample.png', width = 6, height = 6.5, units = 'in')
```


```{r}
c.norm <- smoothed %>%
  arrange(doy) %>%
  pivot_wider(names_from = doy, values_from = smoothed)

colSums(is.na(c.norm))

missing <- c.norm[rowSums(is.na(c.norm)) > 0,]
c.norm <- c.norm %>% filter(!Hylak_id %in% missing$Hylak_id)


c.norm.sf <- hl %>% inner_join(c.norm) %>% distinct(Hylak_id, .keep_all = T)
set.seed(4235)
sample <- c.norm.sf %>% group_by(region) %>% sample_frac(.15)
#sample <- c.norm  %>% distinct(Hylak_id) %>% sample_n(1500)
sample <- c.norm  %>% filter(Hylak_id %in% sample$Hylak_id)

##Take a quick look at the distribution
#sample %>% inner_join(hl) %>% st_as_sf() %>% mapview::mapview(.)

cluster_dtw<-tsclust(smoothed %>% select(-Hylak_id, -period), 
                     type = "p", k = 2L:5L,#2L:8L, 
                     distance = "dtw_basic",
                     centroid = 'dba',
                     #window.size = 2L,
                     #control = hierarchical_control(method = 'complete'),
                     control = partitional_control(iter.max = 400L),
                     trace = T,
                     args = tsclust_args(dist = list(window.size = 2L),
                                         cent = list(window.size = 2L)),
                     preproc = NULL)

# extract the cluster validation indices
# The vignette to dtwclust mentions that CH and SF may not be appropriate with 
# dba centroids, so we'll exclude them.


cvi_dw <-sapply(cluster_dtw, cvi, type = c('DB','DBstar', 'CH','SF'))#c('Sil', 'D', 'COP', 'DB', 'DBstar'))# "valid") 

cvi_names <-rownames(cvi_dw)

cvi_df <- cvi_dw %>%
  as_tibble() %>%
  cbind(cvi_names) %>%
  pivot_longer(-cvi_names, names_to="clusters", names_prefix = "V",values_to = "CVI" ) %>%
  mutate(clusters = as.numeric(clusters) +1)

cvi.selects <- cvi_df %>% group_by(cvi_names) %>%
  mutate(select= ifelse(cvi_names %in% c('CH', 'D', 'SF', 'Sil'), clusters[CVI == max(CVI)], clusters[CVI == min(CVI)]),
         CVI = ifelse(cvi_names %in% c('CH', 'D', 'SF', 'Sil'), max(CVI), min(CVI))) %>%
  select(-clusters) %>% distinct(cvi_names, .keep_all = T)

ggplot(cvi_df) +
  geom_line(aes(x=clusters, y=CVI)) +
  geom_point(data = cvi.selects, aes(x = select, y = CVI)) +
  facet_wrap(~cvi_names, scales="free")

# The vignette to dtwclust mentions that CH and SF may not be appropriate with 
# dba centroids, so that leaves us with COP-8 (big drop 4), D-8, DB-7,DB*-4/5, Sil-3
plot(cluster_dtw[[2]], type = "centroid") 

#We'll use 3 for parsimony
cluster <- cluster_dtw[[2]]
save(cluster, cvi_df, file = paste0('data/out/',iterations,'_SummerClusterings.RData'))
load()

# "CH" (~): Calinski-Harabasz index (Arbelaitz et al. (2013); to be maximized).
# "COP" (!): COP index (Arbelaitz et al. (2013); to be minimized).
# "D" (!): Dunn index (Arbelaitz et al. (2013); to be maximized).
# "DB" (?): Davies-Bouldin index (Arbelaitz et al. (2013); to be minimized).
# "DBstar" (?): Modified Davies-Bouldin index (DB*) (Kim and Ramakrishna (2005); to be minimized).
# "SF" (~): Score Function (Saitta et al. (2007); to be maximized; see notes).
# "Sil" (!): Silhouette index (Rousseeuw (1987); to be maximized).

clusters.sf <- hl %>% inner_join(
  tibble(Hylak_id = c.norm$Hylak_id, period = c.norm$period, 
         cluster = predict(cluster_dtw[[2]], c.norm %>% ungroup() %>% 
                             select(-c(Hylak_id, period)))))

clusters.sf <- hl %>% inner_join(
  tibble(Hylak_id = smoothed$Hylak_id, period = smoothed$period, 
         cluster = cluster_dtw[[2]]@cluster))

usa <- maps::map('usa', plot = F) %>% st_as_sf() %>% st_transform(st_crs(hl)) 

ggplot() + 
  geom_sf(data = usa) + 
  geom_sf(data = clusters.sf, aes(color = factor(cluster), geometry = geometry), size = .5) +
  facet_wrap(~period)

mapview::mapview(clusters.sf %>% filter(period == '(2008,2020]', cluster %in% c(2,3,4)), zcol = 'cluster')

lakeSamp.base <- merge(
  srCor[month %in% c(5:10) & Hylak_id %in% unique(smoothed$Hylak_id),
        .(Hylak_id, period, week, date, dWL, doy)],
  as.data.table(clusters.sf %>% select(Hylak_id, period, cluster)))

iqrs <- lakeSamp.base[,.(iqr75 = quantile(dWL, .75), 
                         iqr25 = quantile(dWL, .25),
                         med_dWL = median(dWL)), by = .(cluster, week)
                      ][,cluster := factor(cluster, labels = c('Bimodal', 'Summer Greening', 'Spring Greening'))]

p1 <- ggplot(iqrs, aes(x = week *7)) + 
  geom_ribbon(aes(ymin = iqr25, ymax = iqr75), fill = 'grey70') +
  geom_line(aes(y = med_dWL)) +
  facet_wrap(~cluster, ncol = 1) + 
  labs(title = 'Raw', y = 'Dominant Wavelength') +
  theme_bw() +
  theme(axis.title = element_blank()) 

clusters.long <- smoothed %>% pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'norm_dWL') %>% as.data.table()

clusters.long <- merge(clusters.long, as.data.table(clusters.sf %>% select(Hylak_id, period, cluster)))

iqrs.norm <- clusters.long[,.(iqr75 = quantile(norm_dWL, .75), 
                         iqr25 = quantile(norm_dWL, .25),
                         med_dWL = median(norm_dWL)), by = .(cluster, doy)
                         ][,doy := as.numeric(doy)
                           ][,cluster := factor(cluster, labels = c('Bimodal', 'Summer Greening', 'Spring Greening'))]

p2 <- ggplot(iqrs.norm, aes(x = doy)) + 
  geom_ribbon(aes(ymin = iqr25, ymax = iqr75), fill = 'grey70') +
  geom_line(aes(y = med_dWL)) +
  facet_wrap(~cluster, ncol = 1) +
  labs(title = 'Normalized') +
  theme_bw() +
  theme(axis.title = element_blank()) 

g <- gridExtra::grid.arrange(p1,p2, ncol = 2, left = 'Dominant Wavelength', bottom = 'Day of Year')

ggsave('figures/ClusterIQRs.png',plot = g, width = 3.5, height = 4, units = 'in')
```

## Look at the stability of each cluster


```{r}
labels.og <- cluster@cluster
x <- smoothed %>% select(-period, -Hylak_id)

Jaccard = function (x, y,z) {
    M.11 = sum(x == z & y == z)
    M.10 = sum(x == z & y != z)
    M.01 = sum(x != z & y == z)
    return (M.11 / (M.11 + M.10 + M.01))
}

x <- s
dtw_wrapper <- function(bootsrapped){
  
  x.resample <- x %>% sample_n(nrow(x), replace = T)
  
  clust.resample<-tsclust(x.resample, 
                     type = "p", k = 3L,#2L:8L, 
                     distance = "dtw_basic",
                     centroid = 'dba',
                     #window.size = 2L,
                     #control = hierarchical_control(method = 'complete'),
                     control = partitional_control(iter.max = 400L),
                     trace = T,
                     args = tsclust_args(dist = list(window.size = 2L),
                                         cent = list(window.size = 2L)),
                     preproc = NULL)

  labels.new <- predict(clust.resample, x) %>% unname()
  
    ## Some hueristic iteration shows that the clusters centroids are always the three distinct groups with similar group sizes, but their order isn't always the same. So re-order based on group size
  
  relabler <- table(labels.og, labels.new) %>% 
    as_tibble() %>% 
    arrange(-n, labels.og) %>%
    distinct(labels.og, .keep_all = T)
  
  labels.new <- factor(labels.new, levels = relabler$labels.new, 
                       labels = relabler$labels.og)
  
  
  tibble(j1 = Jaccard(labels.og, labels.new, 1),
  j2 = Jaccard(labels.og, labels.new, 2),
  j3 = Jaccard(labels.og, labels.new, 3),
  round = bootsrapped)
}

j.stability <- c(1:100) %>% map_dfr(dtw_wrapper)

j.summary <- j.stability %>% pivot_longer(-round, names_to = 'cluster', values_to = 'score') %>% group_by(cluster) %>% summarise(mean.Jaccard = mean(score, na.rm = T), sd.Jaccard = sd(score, na.rm = T))

```

## Look at spatial distribution of clusters

```{r}

## By period
usa <- maps::map('usa', plot = F) %>% st_as_sf() %>% st_transform(st_crs(hl)) 

grid <- st_make_grid(usa, cellsize = c(100000,100000), square = F) %>% st_as_sf() %>% mutate(ID = row_number())
 
grid <- grid %>% st_join(clusters.sf, left = F)


Modes <- function(x) {
  ux <- unique(x)
  tab <- tabulate(match(x, ux))
  mode <- ux[tab == max(tab)]
  mode <- ifelse(length(mode) > 1, 'mixed', as.character(mode))
  return(as.factor(mode))
}

modalClust <- grid %>% st_set_geometry(NULL) %>%
  group_by(ID, period) %>%
  summarise(modalClust = Modes(cluster))

grid <- grid %>% inner_join(modalClust %>% mutate(modalClust = factor(modalClust, levels = c('1','2','3','mixed'), labels = c('Bimodal', 'Summer Greening', 'Spring Greening', 'Mixed'))))

ggplot() + 
  geom_sf(data = usa) +
  geom_sf(data = grid, aes(fill = modalClust)) +
  scale_fill_viridis_d() +
  ggthemes::theme_map(base_family = 'Arial', base_size = 12) +
  theme(legend.position = 'bottom') +
  facet_wrap(~period)

ggsave('figures/DominantCluster.png', width = 6.5, height = 4, units = 'in')


## Overall
grid <- st_make_grid(usa, cellsize = c(50000,50000), square = F) %>% st_as_sf() %>% mutate(ID = row_number())
 
grid <- grid %>% st_join(clusters.sf, left = F)

modalClust <- grid %>% st_set_geometry(NULL) %>%
  group_by(ID) %>%
  summarise(modalClust = Modes(cluster))

grid <- grid %>% inner_join(modalClust %>% mutate(modalClust = factor(modalClust, levels = c('1','2','3','mixed'), labels = c('Bimodal', 'Summer Greening', 'Spring Greening', 'Mixed'))))

ggplot() + 
  geom_sf(data = usa) +
  geom_sf(data = grid, aes(fill = modalClust)) +
  scale_fill_viridis_d('Modal\nCluster', labels = c('Bimodal', 'Summer\nGreening','Spring\nGreening', 'Multiple\nModes'), option = 'plasma') +
  ggthemes::theme_map(base_size = 11) +
  theme(legend.position = 'bottom')

ggsave('figures/DominantCluster.png', width = 4.5, height = 4, units = 'in')

```


## Try to make a transition plot

```{r}
links.base <- clusters.sf %>% st_set_geometry(NULL) %>%
  mutate(cluster = paste0('Cluster_',cluster)) %>%
  #mutate(pClust = paste0('c',cluster,'_',period), count = 1) %>%
  select(Hylak_id, period, cluster) %>%
  arrange(period) %>%
  pivot_wider(names_from = period, values_from = cluster) 

links.base[is.na(links.base)] = 'None'

p1Tab <- as.data.frame(table('source' = links.base[[2]],'target' = links.base[[3]])) %>%
  mutate(source = paste0('p1_',source), target = paste0('p2_',target))

p2Tab <- as.data.frame(table('source' = links.base[[3]],'target' = links.base[[4]])) %>%
  mutate(source = paste0('p2_',source), target = paste0('p3_',target))

p3Tab <- as.data.frame(table('source' = links.base[[4]],'target' = links.base[[5]])) %>%
  mutate(source = paste0('p3_',source), target = paste0('p4_',target))

p4Tab <- as.data.frame(table('source' = links.base[[5]],'target' = links.base[[6]])) %>%
  mutate(source = paste0('p4_',source), target = paste0('p5_',target))

p5Tab <- as.data.frame(table('source' = links.base[[6]],'target' = links.base[[7]])) %>%
  mutate(source = paste0('p5_',source), target = paste0('p6_',target))

links <- bind_rows(p1Tab,p2Tab,p3Tab,p4Tab,p5Tab)

nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  ) %>%
  mutate(nodeGroup = ifelse(grepl('_1',name), 1, 
                            ifelse(grepl('_2',name), 2, 
                                   ifelse(grepl('_3',name),3,
                                          ifelse(grepl('_4',name),4,
                                                 ifelse(grepl('_5',name),5,
                                                              ifelse(grepl('_6',name),6,7)))))),
         nodeGroup = factor(nodeGroup))
 
# With networkD3, connection must be provided using id, not using real name like in the links dataframe.. So we need to reformat it.
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1
 
# Make the Network

cs <- 'd3.scaleOrdinal() .range(["#440154FF", "#2A788EFF", "#7AD151FF"])'
p <- sankeyNetwork(Links = links, Nodes = nodes,
                     Source = "IDsource", Target = "IDtarget",
                     Value = "Freq", NodeID = "name", NodeGroup = 'nodeGroup',
                     sinksRight=F, nodeWidth = 10, nodePadding = 30,
                   colourScale = cs, fontSize = 0, fontFamily = 'Arial')

p
 
links.base <- links.base %>%
  mutate(change1 = ifelse(`(1984,1990]` == `(1990,1996]`, 0,1),
         change2 = ifelse(`(1990,1996]` == `(1996,2002]`, 0,1),
         change3 = ifelse(`(1996,2002]` == `(2002,2008]`, 0,1),
         change4 = ifelse(`(2002,2008]` == `(2008,2014]`, 0,1),
         change5 = ifelse(`(2008,2014]` == `(2014,2020]`, 0,1),
         totalChange = factor(change1 + change2 + change3 + change4 + change5))

summary(links.base$totalChange)

stability <- links.base %>% 
  pivot_longer(`(1984,1990]`:`(2014,2020]`, names_to = 'Period', 
               values_to = 'Cluster') %>% 
  group_by(Hylak_id) %>% 
  summarize(States = factor(length(unique(Cluster)), 
                            levels = c(1:3), labels = c(1:3))) %>%
  left_join(links.base %>% select(Hylak_id, totalChange))
 
links.sf <- stability %>% inner_join(hl) %>% st_as_sf() 
links.base <- links.sf %>% st_set_geometry(NULL)
mapview::mapview(links.sf, zcol = 'States')

var = 'Depth_avg'

wilcox.test(links.base[[var]][links.base$States == 1], links.base[[var]][links.base$States == 3])

p1 <- ggplot(links.sf, aes(x = totalChange, y = Vol_total)) +
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Volume (km3)') + 
  coord_cartesian(ylim = c(.1,50)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size=4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p2 <- ggplot(links.sf, aes(x = totalChange, y = Res_time)) + 
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Residence Time') + 
  coord_cartesian(ylim = c(10,10000)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size=4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p3 <- ggplot(links.sf, aes(x = totalChange, y = pop_sum_mean)) + 
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  scale_y_continuous(trans = 'log10') +
  labs(x = 'Stability', y = 'Catchment Population') + 
  coord_cartesian(ylim = c(50,15000)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size=4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p4 <- ggplot(links.sf, aes(x = totalChange, y = Slope_100)) + 
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  #scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Surrounding Slope') + 
  coord_cartesian(ylim = c(.5,4)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size= 4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p5 <- ggplot(links.sf, aes(x = totalChange, y = Depth_avg)) + 
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  #scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Average Depth (m)') + 
  coord_cartesian(ylim = c(2,8)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size= 4, color="red", fill="red") +
  theme(axis.title.x = element_blank())

p6 <- ggplot(links.sf, aes(x = totalChange, y = total_precip_mm_mean)) +
  geom_violin(draw_quantiles = c(0.25, 0.75),
  linetype = "dashed") +
  geom_violin(fill="transparent",draw_quantiles = 0.5) +
  scale_y_continuous(trans = 'log10') + 
  labs(x = 'Stability', y = 'Mean Annual Precip (mm)') + 
  coord_cartesian(ylim = c(3000, 10000)) +
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size = 4, color="red", fill="red")  +
  theme(axis.title.x = element_blank())

g <- gridExtra::grid.arrange(p2,p3,p5,p6, bottom = 'Stability')

ggsave('figures/LakeLandscapeMetrics.png', plot = g, width = 4, height = 4, units = 'in')
## Bring in the GLCP
#glcp <- read_csv('data/in/glcp.csv')
#glcp.filt <- as.data.table(glcp)[Hylak_id %in% unique(smoothed$Hylak_id)]
#write_feather(glcp.filt, 'data/in/glcpFiltered.feather')
glcp <- read_feather('data/in/glcpFiltered.feather') %>%
  mutate(spRatio = seasonal_km2/permanent_km2)

glcp.avg <- glcp %>% group_by(Hylak_id) %>% summarise_at(vars(total_precip_mm:pop_sum, seasonal_km2, spRatio), c(mean = mean, sd = sd), na.rm = T)

links.sf <- links.sf %>% left_join(glcp.avg)


## Take another stab at her incorporating number of states too.
ggplot(links.sf) + geom_bar(aes(x = totalChange, fill = States))
ggplot(links.sf) + geom_sf(aes(color = as.numeric(States))) +
  scale_color_viridis_c()


usa <- maps::map('usa', plot = F) %>% st_as_sf() %>% st_transform(st_crs(hl)) 

grid <- st_make_grid(usa, cellsize = c(50000,50000), square = F) %>% st_as_sf() %>% mutate(ID = row_number())
 
grid <- grid %>% st_join(links.sf, left = F)

modalClust <- grid %>% st_set_geometry(NULL) %>%
  group_by(ID) %>%
  summarise(modalClust = Modes(totalChange))

modalClust <- grid %>% inner_join(modalClust %>% mutate(modalClust = factor(modalClust, levels = c('0','1','2','3','4','5','mixed'), labels = c('0','1','2','3','4','5','mixed'))))

ggplot() + 
  geom_sf(data = usa) +
  geom_sf(data = modalClust, aes(fill = modalClust)) +
  scale_fill_manual(values = c(br(6), 'grey30'), 'Mode State \nChanges') +
  ggthemes::theme_map(base_size = 11) +
  theme(legend.position = 'bottom')



ggplot(links.sf, aes(x = totalChange, y = total_precip_mm_mean)) + geom_boxplot() + scale_y_continuous(trans = 'log10') + labs(x = 'Stability') + coord_cartesian(ylim = c(3000,10000))
```

```{r}
library(randomForest)
df.rf <- links.base %>%
  select(totalChange, Lake_type, Lake_area:Wshd_area, region) %>%
  mutate(Lake_type = factor(Lake_type),
         uniqueID = row_number())

train <- df.rf %>% group_by(totalChange) %>% sample_frac(.7)
test <- df.rf %>% filter(!uniqueID %in% train$uniqueID)
train <- train %>% select(-uniqueID)

rf = randomForest(totalChange ~ ., data=train, ntree=100, mtry=2, importance=TRUE)
rf
varImpPlot(rf)

prediction_for_table <- predict(rf, test %>% select(-uniqueID, States))


caret::confusionMatrix(prediction_for_table, test$States)

```
