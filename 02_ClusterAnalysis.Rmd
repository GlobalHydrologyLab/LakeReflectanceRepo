---
title: "02_Cluster Analysis"
author: "Simon Topp"
date: "8/10/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(purrr)
library(furrr)
library(data.table)
library(feather)
library(sf)
library(dtwclust)
library(networkD3)
library(tictoc)
knitr::opts_chunk$set(echo = TRUE)
```
## Try pulling out the smoothed trend lines using doy for each period

```{r}
srCor <- read_feather('data/processed/srCorrected_us_hydrolakes_dp_20200628.feather')

iterations = 'May_Oct'
## Play around with some clustering

## Take a representative sample of HydroLakes
Ecoregs <- st_read('../USLakeClarityTrendr/in/NLA/NLA_Ecoregions/EcoRegsMerged.shp')

dp <- st_read('data/in/HydroLakes_DP/HydroLakes_DP.shp')

hl <- st_read('../USLakeClarityTrendr/in/hydroLakes/HydroLAKES_polys_v10_shp/HydroLAKES_polys_v10.shp') %>%
  filter(Hylak_id %in% dp$Hylak_id) %>%
  st_centroid()


srCor <- as.data.table(srCor)[,period := cut(year,6, dig.lab = 4)
                                ][,doy := yday(date)
                                  ][, week := ifelse(week  == 53,52,week)
                                    ][,biWeek := ifelse(week %% 2 == 0,
                                                         week, week + 1)]

```


```{r}
##  Take a look at biWeek distributions to decide cut-off's for 'Summer'
#hist(srCor$biWeek)
#Looks like it should be ~18 and 38, or ~May-Oct
#summary(factor(srCor$biWeek))

FilterCounts <- srCor[month %in% c(5:10)
                      ][, .N, by = .(Hylak_id, period, month)]

# Find all possible combos to make sure its exhaustive
combos <- as.data.table(expand.grid(month = unique(FilterCounts$month),
                                    Hylak_id = unique(srCor$Hylak_id), 
                                    period = unique(FilterCounts$period)))

# Get rid of months with < 2 Observations
Filter <- merge(FilterCounts,combos, all = T)[is.na(N) | N < 2]

# Get rid of big outliers and scale dWL
lakeSamp <- srCor[month %in% c(5:10) & !Hylak_id %in% unique(Filter$Hylak_id),
                  .(Hylak_id, period, date, dWL, doy)
                  ][,dWL.scaled := scale(dWL), by = .(Hylak_id, period)
                    ][dWL.scaled < 4 & dWL.scaled > -4]
```



```{r}
## This leaves us with ~26,000 lakes with at least 2 observations per month
## in each of the 6 periods
length(unique(lakeSamp$Hylak_id))

## Look at the distribution
lakeSamp %>% distinct(Hylak_id) %>% inner_join(hl) %>% st_as_sf() %>%mapview::mapview()

min(lakeSamp$doy)
max(lakeSamp$doy)

# Create a function to pass a gaussian smoother over each perdiod/lake combo
# to get 1 observation per weak.
k.smoother.summer <- function(data, period, Hylak_id){
 if(type == 'norm'){
  k <- ksmooth(data$doy, data$dWL.scaled, kernel = 'normal', 
                   bandwidth = 21, range.x = seq(min(lakeSamp$doy),max(lakeSamp$doy)), 
                   x.points = seq(min(lakeSamp$doy),max(lakeSamp$doy),7))
 }else if(type == 'raw'){
   k <- ksmooth(data$doy, data$dWL, kernel = 'normal', 
                   bandwidth = 21, range.x = seq(min(lakeSamp$doy),max(lakeSamp$doy)), 
                   x.points = seq(min(lakeSamp$doy),max(lakeSamp$doy),7))
 }
 
 tibble(period = period, Hylak_id = Hylak_id, doy = k$x, smoothed = k$y) %>%
  arrange(doy) %>%
  pivot_wider(names_from = doy, values_from = smoothed)
}


# Nest it all as a data.table
.nest <- function(...) list(data.table(...))

smoothed <- lakeSamp[, .(data = .nest(.SD)),  by = .(period, Hylak_id)]

## Heads up, this takes about 25 minutes
type = 'norm'
tic()
plan(multiprocess, workers = 4)
smoothed.norm <- smoothed %>% future_pmap_dfr(k.smoother.summer, .progress = T)
plan(sequential)
toc()

type = 'raw'
tic()
plan(multiprocess, workers = 4)
smoothed.raw <- smoothed %>% future_pmap_dfr(k.smoother.summer, .progress = T)
plan(sequential)
toc()

rm(smoothed, type)
write_feather(smoothed.norm, paste0('data/out/',iterations,'_Wide_Normalized.feather'))
write_feather(smoothed.raw, paste0('data/out/',iterations,'_Wide_Raw.feather'))
```

## Looking at Tahoe and Crater above it becomes obvious that there are some lakes that 
## are monotonic (e.g. aseasonal).  Create an apriori cluster for them

```{r}
## Test clusters with just de-meaned parts
smoothed.raw.var <- smoothed.raw %>%
  pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'dWL') %>%
  group_by(Hylak_id, period) %>%
  summarise(sd = sd(dWL),
            dWL = as.integer(mean(dWL))) %>%
  left_join(fui.lookup)

quantile(smoothed.raw.var$dWL, 0.01)
quantile(smoothed.raw.var$dWL, 0.99)

end.members <- smoothed.raw.var %>% filter(dWL < 488 | dWL > 574)
nrow(end.members %>% filter(sd <= 5))/nrow(end.members)
nrow(smoothed.raw.var %>% filter(sd <= 5))/nrow(smoothed.raw.var)
hist(end.members$sd)

smoothed.raw.var %>%
  ggplot() + 
  geom_point(aes(x = dWL, y = sd, color = fui)) +
  geom_hline(aes(yintercept = 5), color = 'red') +
  scale_color_gradientn(colors = fui.colors) +
  labs(x = expression(Mean ~ lambda[d]),  y = expression(Standard ~ Deviation ~ lambda[d])) +
  theme_bw() +
  theme(legend.position = 'none') +
  coord_cartesian(xlim = c(470,590))

ggsave('figures/MeanSD_dWL.png', width = 3, height = 3, units = 'in')

aseasonal = smoothed.raw.var %>%
  filter(sd <= 5) %>%
  mutate(lpID = paste0(Hylak_id, period))

## Check which of our example timeseries were classified as aseasonal
aseasonal %>% filter(Hylak_id %in% vis.samp$Hylak_id) %>% left_join(vis.samp)
```


```{r}
smoothed.norm <- read_feather(paste0('data/out/',iterations,'_Wide_Normalized.feather'))
smoothed.raw <- read_feather(paste0('data/out/',iterations,'_Wide_Raw.feather'))

## Version to of figure using known lakes
vis.samp <- tibble(LakeName = c('Mendota (WI)', 'Okeechobee (FL)', 'Crater (OR)'), Hylak_id = c(9086, 69, 9092))
# Lake name, Hylak_id
# Trout lake, 8736
# Mendota, 9086
# Okeechobee, 69	
# Tahoe, 792
# Crater, 9092

# Look at de-meaned but not scaled results
vis.raw <- smoothed.raw %>% filter(Hylak_id %in% vis.samp$Hylak_id) %>%
  pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'dWL') %>%
  #group_by(Hylak_id, period) %>%
  #mutate(dWL = scale(dWL, scale = F)) %>%
  #ungroup() %>%
  mutate(doy = as.numeric(doy)) %>%
  left_join(vis.samp)


vis.norm <- smoothed.norm %>% filter(Hylak_id %in% vis.samp$Hylak_id) %>%
  pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'dWL') %>%
  mutate(doy = as.numeric(doy)) %>%
  left_join(vis.samp)


points <- lakeSamp %>% filter(Hylak_id %in% vis.samp$Hylak_id) %>%
  mutate(dWL = as.integer(dWL)) %>%
  left_join(fui.lookup) %>%
  left_join(vis.samp) %>%
  mutate(fui = ifelse(dWL > 583, 21, ifelse(dWL < 471,1, fui)))

aseas.labs <- aseasonal %>%
  filter(Hylak_id %in% vis.samp$Hylak_id) %>% left_join(vis.samp) %>%
  select(period, LakeName) %>%
  mutate(lab = '*', doy = 125, dWL = 590)

ggplot(vis.raw, aes(x = doy)) + 
  geom_point(data = points, 
             aes(color = fui, y = dWL, fill = 'Landsat Observed Color')) +
  geom_line(size = 1, aes(y = dWL, shape = 'Kmeans Climatology'),color = 'red', show.legend = T) +
  scale_color_gradientn(colors = c(fui.colors), guide = 'none') +
  # scale_color_identity(guide = 'legend',
  #                      name = '',
  #                      breaks = c('black', 'red'),
  #                      labels = c('Observed Color','Kmeans Climatology')) +
  theme_bw() +
  coord_cartesian(ylim = c(450,600)) +
  theme(legend.position = 'top', legend.title = element_blank()) +
  labs(x = 'Day of Year', y = expression(lambda[d])) +
  facet_grid(LakeName~period, labeller = labeller(LakeName = label_wrap_gen(10))) +
  geom_text(data = aseas.labs, aes(x = doy, y = dWL, label = lab), color = 'red', size = 5)


ggsave('figures/ClimatologyExample_V2.png', width = 6.5,height= 4,  units = 'in')

ggplot(vis.norm, aes(x = doy)) + 
  geom_point(data = points, 
             aes(color = fui, y = dWL.scaled, fill = 'Landsat Observed Color')) +
  geom_line(size = 1, aes(y = dWL, shape = 'Kmeans Climatology'),color = 'red', show.legend = T) +
  scale_color_gradientn(colors = c(fui.colors), guide = 'none') +
  # scale_color_identity(guide = 'legend',
  #                      name = '',
  #                      breaks = c('black', 'red'),
  #                      labels = c('Observed Color','Kmeans Climatology')) +
  theme_bw() +
  coord_cartesian(ylim = c(-2,2)) +
  theme(legend.position = 'top', legend.title = element_blank()) +
  labs(x = 'Day of Year', y = expression(Z-Normalized ~ lambda[d])) +
  facet_grid(LakeName~period, labeller = labeller(LakeName = label_wrap_gen(10)))

ggsave('figures/ClimatologyExample_Norm.png', width = 6.5,height= 5,  units = 'in')


rm(vis.samp, vis.raw, vis.norm, points)
```


## Conduct the actual clustering analysis

```{r}
## Check for missing values
colSums(is.na(smoothed.norm))

## Remove aseasonal lakes
lakes.in <- smoothed.norm %>%
  mutate(lpID = paste0(Hylak_id, period)) %>%
  filter(!lpID %in% aseasonal$lpID) %>%
  select(-lpID)

##Take a quick look at the distribution
#smooothed.sf <- hl %>% inner_join(smoothed) %>% distinct(Hylak_id, .keep_all = T)
#mapview(smoothed.sf)

cluster_dtw<-tsclust(lakes.in %>% select(-Hylak_id, -period), 
                     type = "p", k = 2L:8L,#2L:8L, 
                     distance = "dtw_basic",
                     centroid = 'dba',
                     #window.size = 2L,
                     #control = hierarchical_control(method = 'complete'),
                     control = partitional_control(iter.max = 600L),
                     trace = T,
                     args = tsclust_args(dist = list(window.size = 2L),
                                         cent = list(window.size = 2L)),
                     preproc = NULL)


# extract the cluster validation indices
# The vignette to dtwclust mentions that CH and SF may not be appropriate with 
# dba centroids, so we'll exclude them. Similarly, Sil, Dunn, and COP, all require the entire
# cross-distance matrix which is way too big with the number of samples we have.
# That leaves DB and Modified DB
cvi_dw <-sapply(cluster_dtw, cvi, type = c('DB','DBstar')) #c('D','Sil', 'D', 'COP', 'DB', 'DBstar')) 

cvi_names <-rownames(cvi_dw)

cvi_df <- cvi_dw %>%
  as_tibble() %>%
  cbind(cvi_names) %>%
  pivot_longer(-cvi_names, names_to="clusters", names_prefix = "V",values_to = "CVI" ) %>%
  mutate(clusters = as.numeric(clusters) +1)

cvi.selects <- cvi_df %>% group_by(cvi_names) %>%
  mutate(select= ifelse(cvi_names %in% c('CH', 'D', 'SF', 'Sil'), clusters[CVI == max(CVI)], clusters[CVI == min(CVI)]),
         CVI = ifelse(cvi_names %in% c('CH', 'D', 'SF', 'Sil'), max(CVI), min(CVI))) %>%
  select(-clusters) %>% distinct(cvi_names, .keep_all = T)

ggplot(cvi_df) +
  geom_line(aes(x=clusters, y=CVI)) +
  geom_point(data = cvi.selects, aes(x = select, y = CVI)) +
  facet_wrap(~cvi_names, scales="free")


## There's some randomness in partitional clustering, so create the final clustering with ideal
## number using a replicable seed
set.seed(4986)
cluster<-tsclust(lakes.in %>% select(-Hylak_id, -period), 
                     type = "p", k = 3L, 
                     distance = "dtw_basic",
                     centroid = 'dba',
                     #window.size = 2L,
                     #control = hierarchical_control(method = 'complete'),
                     control = partitional_control(iter.max = 600L),
                     trace = T,
                     args = tsclust_args(dist = list(window.size = 2L),
                                         cent = list(window.size = 2L)),
                     preproc = NULL)

# Take a look at the resulting best cluster
cluster@centroids %>% as_tibble(.name_repair = "universal") %>% mutate(doy = seq(1,n())*7+100) %>%
  pivot_longer(-doy, names_to = 'cluster') %>%
  ggplot() + geom_line(aes(x = doy, y = value)) + facet_wrap(~cluster)


iterations <- 'May_Oct_Aseas'
save(cluster, cvi_df, file = paste0('data/out/',iterations,'_SummerClusterings.RData'))
rm(cluster_dtw, cvi_names, cvi_dw, cvi.selects, cvi_df)
```


## Look at the stability of each cluster

```{r}
iterations = 'May_Oct_Aseas'
load(paste0('data/out/',iterations,'_SummerClusterings.RData'))
labels.og <- cluster@cluster
summary(factor(labels.og))

Jaccard = function (x,y,z) {
    M.11 = sum(x == z & y == z)
    M.10 = sum(x == z & y != z)
    M.01 = sum(x != z & y == z)
    return (M.11 / (M.11 + M.10 + M.01))
}

dtw_wrapper <- function(bootsrapped){
  
  x.resample <- lakes.in %>% select(-Hylak_id, -period) %>% sample_n(nrow(lakes.in), replace = T)
  
  clust.resample<-tsclust(x.resample, 
                     type = "p", k = 3L, 
                     distance = "dtw_basic",
                     centroid = 'dba',
                     control = partitional_control(iter.max = 400L),
                     trace = T,
                     args = tsclust_args(dist = list(window.size = 2L),
                                         cent = list(window.size = 2L)),
                     preproc = NULL)

  labels.new <- predict(clust.resample, lakes.in %>% select(-Hylak_id, -period)) %>% unname()
  
  ## Some hueristic iteration shows that the clusters centroids are always the three distinct groups with similar group sizes, but their order isn't always the same. So some tricky re-ordering/labelling is needed
  
  relabler <- table(labels.og, labels.new) %>% 
    as_tibble() %>% 
    #mutate(frac = ifelse(labels.og == 1, n/23578, ifelse(labels.og == 2, n/40504, n/45561))) %>% 
    arrange(-n) 
    
  relabled <- relabler[1,] 
  relabled <- relabled %>% bind_rows(
    relabler %>% filter(labels.og != relabled$labels.og & labels.new != relabled$labels.new) %>%
    arrange(-n) %>% .[1,])
  relabled <- relabled %>% bind_rows(
    relabler %>% filter(!labels.og %in% relabled$labels.og & !labels.new %in% relabled$labels.new) %>%
      arrange(-n) %>% .[1,])
  
  labels.new <- factor(labels.new, levels = relabled$labels.new, 
                       labels = relabled$labels.og)
  
  tibble(SpGr = Jaccard(labels.og, labels.new, 1),
  SuGr = Jaccard(labels.og, labels.new, 3),
  Bi = Jaccard(labels.og, labels.new, 2),
  round = bootsrapped,
  converged = clust.resample@converged)
}

j.stability <- c(1:100) %>% map_dfr(dtw_wrapper)

write_feather(j.stability, paste0('data/out/JaccardStability', iterations,'.feather'))

j.summary <- j.stability %>% pivot_longer(-c(round, converged), names_to = 'cluster', values_to = 'score') %>% group_by(cluster) %>% summarise(mean.Jaccard = mean(score, na.rm = T), sd.Jaccard = sd(score, na.rm = T))
```


```{r}
load(paste0('data/out/',iterations,'_SummerClusterings.RData'))

clusters.sf <- hl %>% inner_join(
  tibble(Hylak_id = lakes.in$Hylak_id, period = lakes.in$period, cluster = cluster@cluster) %>%
    bind_rows(aseasonal %>% ungroup() %>% select(Hylak_id, period) %>% mutate(cluster = 4)) %>% 
    mutate(cluster = factor(cluster, levels = c(1,3,2,4), #c(3,2,1,4), 
                          labels = c('Spring Greening', 'Summer Greening', 'Bimodal', 'Aseasonal'))))

              
#st_write(clusters.sf, paste0('data/out/',iterations,'_clustersSF.shp'), delete_layer = TRUE)

#Visualize the clusters

clusters.raw.long <- smoothed.raw %>% pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'dWL') %>% as.data.table()

clusters.raw.long <- merge(clusters.raw.long, as.data.table(clusters.sf %>% select(Hylak_id, period, cluster)))

clusters.raw.long <- merge(clusters.raw.long, fui.lookup, by = 'dWL')

iqrs.raw <- clusters.raw.long[,.(iqr75 = quantile(dWL, .75), 
                         iqr25 = quantile(dWL, .25),
                         med_dWL = as.integer(median(dWL))), by = .(cluster, doy)
                         ][,doy := as.numeric(doy)]

getDense <- function(cl){
  df <- clusters.raw.long[clusters.raw.long$cluster == cl]
  dense <- density(df$dWL, n = 200)
  tibble(cluster = cl, dWL = as.integer(dense$x), dense = dense$y) %>% inner_join(fui.lookup, by = 'dWL')
  }

ColorDense = unique(clusters.raw.long$cluster) %>% map_dfr(getDense) %>%
  mutate(cluster = factor(cluster, levels = c('Spring Greening', 'Summer Greening', 'Bimodal', 'Aseasonal')))
  

p1 <- ggplot(ColorDense, aes(x = dWL, y = dense, color = fui)) +
  geom_segment(aes(xend = dWL, yend = 0), size = 2) +
  scale_color_gradientn(colors = fui.colors) +
  labs(tag = 'c', x = expression(lambda[d]), title = 'Density') +
  theme(axis.line.y = element_blank(),
      axis.ticks = element_blank(),
      #axis.text.x = element_text(vjust = 5),
      axis.text.y = element_blank(),
      axis.title.y = element_blank(),
      panel.background = element_blank(),
      panel.grid = element_blank(),
      legend.position = 'none') +
  facet_grid(rows = vars(cluster))


p2 <- ggplot(iqrs.raw, aes(x = doy)) + 
  geom_ribbon(aes(ymin = iqr25, ymax = iqr75), fill = 'grey70') +
  geom_line(aes(y = med_dWL)) +
  facet_wrap(~cluster, ncol = 1) +
  labs(tag = 'b', title = 'Raw') +
  theme_bw() +
  theme(axis.title = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())

###############
clusters.long <- smoothed.norm %>% pivot_longer(-c(Hylak_id, period), names_to = 'doy', values_to = 'norm_dWL') %>% as.data.table()

clusters.long <- merge(clusters.long, as.data.table(clusters.sf %>% select(Hylak_id, period, cluster)))

iqrs.norm <- clusters.long[,.(iqr75 = quantile(norm_dWL, .75), 
                         iqr25 = quantile(norm_dWL, .25),
                         med_dWL = median(norm_dWL)), by = .(cluster, doy)
                         ][,doy := as.numeric(doy)]

p3 <- ggplot(iqrs.norm, aes(x = doy)) + 
  geom_ribbon(aes(ymin = iqr25, ymax = iqr75), fill = 'grey70') +
  geom_line(aes(y = med_dWL)) +
  facet_wrap(~cluster, ncol = 1) +
  labs(tag = "a", title = 'Z-Normalized', y = expression(lambda[d])) +
  theme_bw() +
  theme(axis.title.x = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())

g1 <- gridExtra::grid.arrange(p3,p2,ncol = 2, widths = c(1.1,1), bottom = 'Day of Year')

g <- gridExtra::grid.arrange(g1, p1, ncol = 2, widths = c(1,.5))

ggsave('figures/ClusterIQRs_v2.png',plot = g, width = 6, height = 5.5, units = 'in')
```

# Pull out some descriptive numbers for each cluster

```{r}
# Just pull some summary values
Modes <- function(x) {
  ux <- unique(x)
  tab <- tabulate(match(x, ux))
  ux[tab == max(tab)][1]
  }

clusters.sf %>% group_by(cluster) %>% summarise(count = n(), percent = count/nrow(clusters.sf))

clusters.raw.long %>% group_by(cluster) %>% 
  summarise(
    mean = mean(dWL),
    median = median(dWL),
    sd = sd(dWL),
    IQR = IQR(dWL),
    mode = Modes(dWL),
    var = var(dWL))

#Summary variation by lake
lake.var <- clusters.raw.long %>% group_by(cluster, period, Hylak_id) %>%
  summarise(median = median(dWL),
            sd = sd(dWL),
            cv = raster::cv(dWL),
            sd.fui = sd(fui),
            cv.fui = raster::cv(fui),
            rangeLow = range(dWL)[1],
            rangeHigh = range(dWL)[2])

lake.var %>%
  group_by(cluster) %>%
  summarise(mean.sd = mean(sd),
            mean.cv = mean(cv),
            sd.sd = sd(sd),
            mean.rl = mean(rangeLow),
            mean.rh = mean(rangeHigh),
            range = mean.rh - mean.rl)


kruskal.test(cv ~ cluster, lake.var)
FSA::dunnTest(cv~cluster, data = lake.var, method = 'bonferroni')

# Look at median color amongst clusters
clusters.raw.long %>% group_by(cluster) %>% summarise(median = median(dWL))
FSA::dunnTest(dWL~cluster, data = clusters.raw.long, method = 'bonferroni')


View(ColorDense %>% filter(cluster == 'Aseasonal'))
View(ColorDense %>% filter(cluster == 'Bimodal'))
```

# Finally, create Aseasonal Blue and Aseasonal Green clusters respectively

```{r}
aBlue <- clusters.raw.long %>% 
  filter(cluster == 'Aseasonal') %>%
  group_by(period, Hylak_id) %>%
  summarise(median = median(dWL, na.rm = T)) %>%
  filter(median < 525) %>%
  mutate(lpID = paste0(Hylak_id, period))

clusters.sf <- clusters.sf %>%
  mutate(lpID = paste0(Hylak_id, period),
         cluster = as.character(cluster),
         cluster = ifelse(lpID %in% aBlue$lpID, 'Aseasonal (Blue)',
                          ifelse(cluster == 'Aseasonal' & !lpID %in% aBlue$lpID, 
                                 'Aseasonal (Green)', cluster)),
         cluster = factor(cluster, levels = c('Spring Greening', 'Summer Greening', 'Bimodal', 'Aseasonal (Blue)', 'Aseasonal (Green)')))

st_write(clusters.sf, paste0('data/out/',iterations,'_clustersSF.shp'), delete_layer = T)
```